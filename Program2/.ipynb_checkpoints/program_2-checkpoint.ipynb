{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d177f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean, stdev\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9569435",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Part one</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c68b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_labels_1 = ['Full set', '95% variance set', '25 PCA set', '16 PCA set', '9 PCA set', '4 PCA set']\n",
    "set_labels_2 = ['Full set', 'Balanced Class Wt.', 'Balanced Subsample Class Wt.', 'Under-sampling', 'Over-sampling', 'Over/Under Combination']\n",
    "\n",
    "set_columns_1 = ['Average Accuracy', 'Accuracy Deviation']\n",
    "set_columns_2 = ['Avg. Prec.', 'Prec. Dev.', 'Avg. Recall', 'Recall Dev.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe5279f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformDataForTable(accuracies):\n",
    "    result = []\n",
    "    result_average = []\n",
    "    result_deviation = []\n",
    "    \n",
    "    for accuracy in accuracies:\n",
    "        result_average.append(round(mean(accuracy), 2))\n",
    "        result_deviation.append(round(stdev(accuracy), 2))\n",
    "        \n",
    "    \n",
    "    result.append(result_average)\n",
    "    result.append(result_deviation)\n",
    "    \n",
    "    transposed = np.transpose(result)\n",
    "    \n",
    "    return transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5542e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionAndRecallCalculator(matricies):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for matrix in matricies:\n",
    "        if(matrix[0, 1] + matrix[1, 1] != 0):\n",
    "            precisions.append(float(matrix[1, 1] / (matrix[0, 1] + matrix[1, 1])))\n",
    "        else:\n",
    "            precisions.append(float(matrix[1, 1]))\n",
    "        if(matrix[1, 0] + matrix[1, 1] != 0):\n",
    "            recalls.append(float(matrix[1, 1] / (matrix[1, 0] + matrix[1, 1])))\n",
    "        else:\n",
    "            recalls.append(float(matrix[1, 1]))\n",
    "        \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f41d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionAndRecallReportGenerator(matricies):\n",
    "    \n",
    "    reports = []\n",
    "    \n",
    "    for matrix in matricies:\n",
    "        precision, recall = PrecisionAndRecallCalculator(matrix)\n",
    "        report = []\n",
    "        report.append(round(mean(precision), 2))\n",
    "        report.append(round(stdev(precision), 2))\n",
    "        report.append(round(mean(recall), 2))\n",
    "        report.append(round(stdev(recall), 2))\n",
    "        reports.append(report)\n",
    "        \n",
    "        \n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a39a551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionAndRecallTableConstuctor(matricies, labels, columns):\n",
    "    df1 = pd.DataFrame( PrecisionAndRecallReportGenerator(matricies),\n",
    "                        index = pd.Index(labels),\n",
    "                        columns = columns)\n",
    "    print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bbfc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReportConstructor(accuracies, labels, columns):\n",
    "    df1 = pd.DataFrame( TransformDataForTable(accuracies),\n",
    "                        index = pd.Index(labels),\n",
    "                        columns = columns)\n",
    "    print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399b7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing arrays \n",
    "accuracy_full = []\n",
    "accuracy_95p = []\n",
    "accuracy_25 = []\n",
    "accuracy_16 = []\n",
    "accuracy_9 = []\n",
    "accuracy_4 = []\n",
    "\n",
    "accuracy_bal = []\n",
    "accuracy_balsub = []\n",
    "accuracy_rus = []\n",
    "accuracy_ros = []\n",
    "accuracy_comb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a7de460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data and separate it into two variables (X - protein readings, Y - labeled cancer type) \n",
    "dataset = pd.read_csv('breast_cancer_genomic.csv')\n",
    "dataset.pop('CLID')\n",
    "\n",
    "y = dataset['Class'].values\n",
    "dataset.pop('Class')\n",
    "x = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6455b23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the PCA classifiers that are needed for the next steps in our program\n",
    "pca_95p = PCA(n_components = 0.95)\n",
    "pca_25 = PCA(n_components = 25)\n",
    "pca_16 = PCA(n_components = 16)\n",
    "pca_9 = PCA(n_components = 9)\n",
    "pca_4 = PCA(n_components = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18573c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing RandomForestTree Classifier \n",
    "rf_params = [{'n_estimators': [10, 50, 100, 200, 400, 800, 1000]}, {'criterion': ['gini', 'entropy']}]\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe56af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = rf_params, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11aac84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing our lists\n",
    "accuracy_full.clear()\n",
    "accuracy_95p.clear()\n",
    "accuracy_25.clear()\n",
    "accuracy_16.clear()\n",
    "accuracy_9.clear()\n",
    "accuracy_4.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e373e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the 10 runs that are needed in order to calculate the model accuracy\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # Modify how PCA works on X as it has to be done before the train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2)\n",
    "    \n",
    "    # Performing all of the necessary data reductions\n",
    "    x_train_95p = pca_95p.fit_transform(x_train)\n",
    "    x_test_95p = pca_95p.transform(x_test)\n",
    "    x_train_25 = pca_25.fit_transform(x_train)\n",
    "    x_test_25 = pca_25.transform(x_test)\n",
    "    x_train_16 = pca_16.fit_transform(x_train)\n",
    "    x_test_16 = pca_16.transform(x_test)\n",
    "    x_train_9 = pca_9.fit_transform(x_train)\n",
    "    x_test_9 = pca_9.transform(x_test)\n",
    "    x_train_4 = pca_4.fit_transform(x_train)\n",
    "    x_test_4 = pca_4.transform(x_test)\n",
    "    \n",
    "    best_estimator_full = grid_search.fit(x_train, y_train)\n",
    "    rf_best_full = best_estimator_full.best_estimator_\n",
    "    rf_best_full.fit(x_train, y_train)\n",
    "    predict_full = rf_best_full.predict(x_test)\n",
    "    accuracy_full.append(rf_best_full.score(x_test, y_test))\n",
    "    \n",
    "    best_estimator_95p = grid_search.fit(x_train_95p, y_train)\n",
    "    rf_best_95p = best_estimator_95p.best_estimator_\n",
    "    rf_best_95p.fit(x_train_95p, y_train)\n",
    "    predict_95p = rf_best_95p.predict(x_test_95p)\n",
    "    accuracy_95p.append(rf_best_95p.score(x_test_95p, y_test))\n",
    "    \n",
    "    best_estimator_25 = grid_search.fit(x_train_25, y_train)\n",
    "    rf_best_25 = best_estimator_25.best_estimator_\n",
    "    rf_best_25.fit(x_train_25, y_train)\n",
    "    predict_25 = rf_best_25.predict(x_test_25)\n",
    "    accuracy_25.append(rf_best_25.score(x_test_25, y_test))\n",
    "    \n",
    "    best_estimator_16 = grid_search.fit(x_train_16, y_train)\n",
    "    rf_best_16 = best_estimator_16.best_estimator_\n",
    "    rf_best_16.fit(x_train_16, y_train)\n",
    "    predict_16 = rf_best_16.predict(x_test_16)\n",
    "    accuracy_16.append(rf_best_16.score(x_test_16, y_test))\n",
    "    \n",
    "    best_estimator_9 = grid_search.fit(x_train_9, y_train)\n",
    "    rf_best_9 = best_estimator_9.best_estimator_\n",
    "    rf_best_9.fit(x_train_9, y_train)\n",
    "    predict_9 = rf_best_9.predict(x_test_9)\n",
    "    accuracy_9.append(rf_best_9.score(x_test_9, y_test))\n",
    "    \n",
    "    best_estimator_4 = grid_search.fit(x_train_4, y_train)\n",
    "    rf_best_4 = best_estimator_4.best_estimator_\n",
    "    rf_best_4.fit(x_train_4, y_train)\n",
    "    predict_4 = rf_best_4.predict(x_test_4)\n",
    "    accuracy_4.append(rf_best_4.score(x_test_4, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "accuracies.append(accuracy_full)\n",
    "accuracies.append(accuracy_95p)\n",
    "accuracies.append(accuracy_25)\n",
    "accuracies.append(accuracy_16)\n",
    "accuracies.append(accuracy_9)\n",
    "accuracies.append(accuracy_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817857d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReportConstructor(accuracies, set_labels_1, set_columns_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7e10b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Part two</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10675340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data\n",
    "for i in range(0, y.size):\n",
    "    if(y[i] == 4):\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the classifiers\n",
    "\n",
    "rf_balanced = RandomForestClassifier(class_weight = 'balanced')\n",
    "grid_search_bal = GridSearchCV(estimator = rf_balanced, param_grid = rf_params, scoring='accuracy')\n",
    "rf_balanced_subsample = RandomForestClassifier(class_weight = 'balanced_subsample')\n",
    "grid_search_balsub = GridSearchCV(estimator = rf_balanced_subsample, param_grid = rf_params, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284aee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilizing random under sampler, random over sampler and their combination\n",
    "under_sampler = RandomUnderSampler(sampling_strategy = 'majority', random_state = 50)\n",
    "over_sampler = SMOTE(sampling_strategy = 'minority', random_state = 100)\n",
    "under_over = Pipeline(steps = [('u', under_sampler), ('o', over_sampler)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through 10 turns\n",
    "confusion_matrices = []\n",
    "matrices_full = []\n",
    "matrices_bal = []\n",
    "matrices_balsub = []\n",
    "matrices_rus = []\n",
    "matrices_ros = []\n",
    "matrices_comb = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2)\n",
    "    \n",
    "    # Full data set\n",
    "    best_estimator_full = grid_search.fit(x_train, y_train)\n",
    "    rf_best_full = best_estimator_full.best_estimator_\n",
    "    rf_best_full.fit(x_train, y_train)\n",
    "    predict_full = rf_best_full.predict(x_test)\n",
    "    matrices_full.append(confusion_matrix(y_true = y_test, y_pred = predict_full))\n",
    "    \n",
    "    # Using class_weight = balanced Random Forest Classifier\n",
    "    best_estimator_bal = grid_search_bal.fit(x_train, y_train)\n",
    "    rf_best_bal = best_estimator_bal.best_estimator_\n",
    "    rf_best_bal.fit(x_train, y_train)\n",
    "    predict_bal = rf_best_bal.predict(x_test)\n",
    "    matrices_bal.append(confusion_matrix(y_true = y_test, y_pred = predict_bal))\n",
    "    \n",
    "    # Using class_weight = balanced_subsample Random Forest Classifier\n",
    "    best_estimator_balsub = grid_search_balsub.fit(x_train, y_train)\n",
    "    rf_best_balsub = best_estimator_balsub.best_estimator_\n",
    "    rf_best_balsub.fit(x_train, y_train)\n",
    "    predict_balsub = rf_best_balsub.predict(x_test)\n",
    "    matrices_balsub.append(confusion_matrix(y_true = y_test, y_pred = predict_balsub))\n",
    "    \n",
    "    # Using random under-sampling\n",
    "    x_train_rus , y_train_rus = under_sampler.fit_resample(x_train, y_train)\n",
    "    best_estimator_rus = grid_search.fit(x_train_rus , y_train_rus)\n",
    "    rf_best_rus = best_estimator_rus.best_estimator_\n",
    "    rf_best_rus.fit(x_train_rus , y_train_rus)\n",
    "    predict_rus = rf_best_rus.predict(x_test)\n",
    "    matrices_rus.append(confusion_matrix(y_true = y_test, y_pred = predict_rus))\n",
    "    \n",
    "    # Using random over-sampling\n",
    "    x_train_ros , y_train_ros = over_sampler.fit_resample(x_train, y_train)\n",
    "    best_estimator_ros = grid_search.fit(x_train_ros , y_train_ros)\n",
    "    rf_best_ros = best_estimator_ros.best_estimator_\n",
    "    rf_best_ros.fit(x_train_ros , y_train_ros)\n",
    "    predict_ros = rf_best_ros.predict(x_test)\n",
    "    matrices_ros.append(confusion_matrix(y_true = y_test, y_pred = predict_ros))\n",
    "    \n",
    "    # Using combination of under-sampling and over-sampling\n",
    "    x_train_comb, y_train_comb = under_over.fit_resample(x_train, y_train)\n",
    "    best_estimator_comb = grid_search.fit(x_train_comb, y_train_comb)\n",
    "    rf_best_comb = best_estimator_comb.best_estimator_\n",
    "    rf_best_comb.fit(x_train_comb, y_train_comb)\n",
    "    predict_comb = rf_best_comb.predict(x_test)\n",
    "    matrices_comb.append(confusion_matrix(y_true = y_test, y_pred = predict_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c91676",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices.append(matrices_full)\n",
    "confusion_matrices.append(matrices_bal)\n",
    "confusion_matrices.append(matrices_balsub)\n",
    "confusion_matrices.append(matrices_rus)\n",
    "confusion_matrices.append(matrices_ros)\n",
    "confusion_matrices.append(matrices_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d74270",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionAndRecallTableConstuctor(confusion_matrices, set_labels_2, set_columns_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81669c8b",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71eaca",
   "metadata": {},
   "source": [
    "> Imbalanced classification is specifically hard because of the severely skewed class distribution and the unequal misclassification costs.\n",
    "This is the cause for poor performance with traditional machine learning models and evaluation metrics that assume a balanced class distribution.\n",
    "\n",
    "> Classification algorithms are unable to make reliable models\n",
    "on the datasets with huge sizes. These datasets contain many\n",
    "irrelevant and redundant features that mislead the classifiers.\n",
    "Furthermore, many huge datasets have imbalanced class\n",
    "distribution which leads to bias over majority class in the\n",
    "classification process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
